{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhalderson1986/Configs/blob/main/stable-diffusion-dreambooth_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JWPXdPVcgIIh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
        "!pip install -q https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.16/xformers-0.0.16+814314d.d20230118-cp38-cp38-linux_x86_64.whl\n",
        "!pip install -q triton==2.0.0\n",
        "!git clone --depth 1 --branch v0.9.0 https://github.com/huggingface/diffusers.git\n",
        "!sed -i -e 's/if self._use_memory_efficient_attention_xformers:/if True:/g' /content/diffusers/src/diffusers/models/attention.py\n",
        "!pip install -q /content/diffusers\n",
        "!pip install -q transformers==4.23.1 ftfy==6.1.1 accelerate==0.13.1 bitsandbytes==0.37.0 safetensors==0.2.8\n",
        "\n",
        "import argparse\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "from contextlib import nullcontext\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import PIL\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/camenduru/sd15.git\n",
        "pretrained_model_name_or_path = \"camenduru/sd15\" #@param {type:\"string\"}\n",
        "\n",
        "instance_prompt = \"dillonhalderson\" #@param {type:\"string\"}\n",
        "prior_preservation = True\n",
        "prior_preservation_class_prompt = \"person\" #@param {type:\"string\"}\n",
        "\n",
        "class_prompt=prior_preservation_class_prompt\n",
        "num_class_images = 12\n",
        "sample_batch_size = 2\n",
        "prior_loss_weight = 1\n",
        "\n",
        "root_folder = \"/content/gdrive/MyDrive/AI/training\" #@param {type:\"string\"}\n",
        "prior_preservation_class_folder = f\"{root_folder}/{prior_preservation_class_prompt}\"\n",
        "class_data_root= prior_preservation_class_folder\n",
        "\n",
        "save_path = f\"{root_folder}/{instance_prompt}\"\n",
        "\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        instance_data_root,\n",
        "        instance_prompt,\n",
        "        tokenizer,\n",
        "        class_data_root=None,\n",
        "        class_prompt=None,\n",
        "        size=512,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.instance_data_root = Path(instance_data_root)\n",
        "        if not self.instance_data_root.exists():\n",
        "            raise ValueError(\"Instance images root doesn't exists.\")\n",
        "\n",
        "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.instance_prompt = instance_prompt\n",
        "        self._length = self.num_instance_images\n",
        "\n",
        "        if class_data_root is not None:\n",
        "            self.class_data_root = Path(class_data_root)\n",
        "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
        "            self.class_images_path = list(Path(class_data_root).iterdir())\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_class_images, self.num_instance_images)\n",
        "            self.class_prompt = class_prompt\n",
        "        else:\n",
        "            self.class_data_root = None\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_data_root:\n",
        "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                self.class_prompt,\n",
        "                padding=\"do_not_pad\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "\n",
        "        return example\n",
        "\n",
        "\n",
        "class PromptDataset(Dataset):\n",
        "    def __init__(self, prompt, num_samples):\n",
        "        self.prompt = prompt\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        example[\"prompt\"] = self.prompt\n",
        "        example[\"index\"] = index\n",
        "        return example\n",
        "\n",
        "import gc\n",
        "if(prior_preservation):\n",
        "    class_images_dir = Path(class_data_root)\n",
        "    if not class_images_dir.exists():\n",
        "        class_images_dir.mkdir(parents=True)\n",
        "    cur_class_images = len(list(class_images_dir.iterdir()))\n",
        "\n",
        "    if cur_class_images < num_class_images:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.float16, safety_checker=None).to(\"cuda\")\n",
        "        pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "        num_new_images = num_class_images - cur_class_images\n",
        "        print(f\"Number of class images to sample: {num_new_images}.\")\n",
        "\n",
        "        sample_dataset = PromptDataset(class_prompt, num_new_images)\n",
        "        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=sample_batch_size)\n",
        "\n",
        "        for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n",
        "            with torch.autocast(\"cuda\"):\n",
        "                images = pipeline(example[\"prompt\"]).images\n",
        "\n",
        "            for i, image in enumerate(images):\n",
        "                image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n",
        "        pipeline = None\n",
        "        gc.collect()\n",
        "        del pipeline\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
        "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
        "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
        "\n",
        "from argparse import Namespace\n",
        "args = Namespace(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    resolution=512,\n",
        "    center_crop=True,\n",
        "    instance_data_dir=save_path,\n",
        "    instance_prompt=instance_prompt,\n",
        "    learning_rate=5e-6,\n",
        "    max_train_steps=650,\n",
        "    train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    max_grad_norm=1.0,\n",
        "    mixed_precision=\"fp16\", # set to \"fp16\" for mixed-precision training.\n",
        "    gradient_checkpointing=False, # set this to True to lower the memory usage.\n",
        "    use_8bit_adam=True, # use 8bit optimizer from bitsandbytes for DeepSpeed turn off (False)\n",
        "    seed=69,\n",
        "    with_prior_preservation=prior_preservation,\n",
        "    prior_loss_weight=prior_loss_weight,\n",
        "    sample_batch_size=sample_batch_size,\n",
        "    class_data_dir=prior_preservation_class_folder,\n",
        "    class_prompt=prior_preservation_class_prompt,\n",
        "    num_class_images=num_class_images,\n",
        "    output_dir=instance_prompt,\n",
        ")\n",
        "\n",
        "def training_function(text_encoder, vae, unet):\n",
        "    logger = get_logger(__name__)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "    )\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "\n",
        "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
        "    if args.use_8bit_adam:\n",
        "        optimizer_class = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_class = torch.optim.AdamW\n",
        "\n",
        "    optimizer = optimizer_class(\n",
        "        unet.parameters(),  # only optimize unet\n",
        "        lr=args.learning_rate,\n",
        "    )\n",
        "\n",
        "    noise_scheduler = DDPMScheduler(\n",
        "        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n",
        "    )\n",
        "\n",
        "    train_dataset = DreamBoothDataset(\n",
        "        instance_data_root=args.instance_data_dir,\n",
        "        instance_prompt=args.instance_prompt,\n",
        "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
        "        class_prompt=args.class_prompt,\n",
        "        tokenizer=tokenizer,\n",
        "        size=args.resolution,\n",
        "        center_crop=args.center_crop,\n",
        "    )\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "\n",
        "        # concat class and instance examples for prior preservation\n",
        "        if args.with_prior_preservation:\n",
        "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "            pixel_values += [example[\"class_images\"] for example in examples]\n",
        "\n",
        "        pixel_values = torch.stack(pixel_values)\n",
        "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "\n",
        "        input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"pixel_values\": pixel_values,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
        "\n",
        "    weight_dtype = torch.float32\n",
        "    if args.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif args.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    # Move text_encode and vae to gpu.\n",
        "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
        "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
        "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(\"Steps\")\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_train_epochs):\n",
        "        unet.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Convert images to latent space\n",
        "                with torch.no_grad():\n",
        "                    latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
        "                    latents = latents * 0.18215\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
        "                ).long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                with torch.no_grad():\n",
        "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                # Predict the noise residual\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                if args.with_prior_preservation:\n",
        "                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n",
        "                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "                    noise, noise_prior = torch.chunk(noise, 2, dim=0)\n",
        "\n",
        "                    # Compute instance loss\n",
        "                    loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                    # Compute prior loss\n",
        "                    prior_loss = F.mse_loss(noise_pred_prior.float(), noise_prior.float(), reduction=\"mean\")\n",
        "\n",
        "                    # Add the prior loss to the instance loss.\n",
        "                    loss = loss + args.prior_loss_weight * prior_loss\n",
        "                else:\n",
        "                    loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item()}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "        accelerator.wait_for_everyone()\n",
        "\n",
        "    # Create the pipeline using using the trained modules and save it.\n",
        "    if accelerator.is_main_process:\n",
        "        pipeline = StableDiffusionPipeline(\n",
        "            text_encoder=text_encoder,\n",
        "            vae=vae,\n",
        "            unet=accelerator.unwrap_model(unet),\n",
        "            tokenizer=tokenizer,\n",
        "            scheduler=PNDMScheduler(\n",
        "                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n",
        "            ),\n",
        "            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "\n",
        "import accelerate\n",
        "accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "import torch, requests\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(args.output_dir, torch_dtype=torch.float16, safety_checker=None).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6OfZB_9hJ8sP"
      },
      "outputs": [],
      "source": [
        "root_folder = '/content/gdrive/MyDrive/AI/trained' #@param {type: 'string'}\n",
        "image_folder = '001' #@param {type: 'string'}\n",
        "if os.path.exists(f\"{root_folder}\") == False:\n",
        "  os.mkdir(f\"{root_folder}\")\n",
        "if os.path.exists(f\"{root_folder}/{image_folder}\") == False:\n",
        "  os.mkdir(f\"{root_folder}/{image_folder}\")\n",
        "name = max([int(f[:f.index('.')]) for f in os.listdir(f\"{root_folder}/{image_folder}\")], default=0)\n",
        "\n",
        "height = 512 #@param {type: 'integer'}\n",
        "width = 512 #@param {type: 'integer'}\n",
        "\n",
        "def generate(prompt, name):\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  with autocast(\"cuda\"):\n",
        "    image = pipeline(prompt, height=height, width=width, num_inference_steps=50, eta=0.0, guidance_scale=7.5).images[0]\n",
        "  image.save(f\"{root_folder}/{image_folder}/{name:04}.png\")\n",
        "  display(image)\n",
        "\n",
        "prompt = \"drawing\" #@param {type: 'string'}\n",
        "full = f\"{instance_prompt} {prior_preservation_class_prompt} {prompt}\"\n",
        "name += 1\n",
        "generate(full, name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ivexhzmla5bh"
      },
      "outputs": [],
      "source": [
        "#@markdown convert to webui\n",
        "!wget -q https://raw.githubusercontent.com/huggingface/diffusers/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "model_name = \"parkminyoung\" #@param {type:\"string\"}\n",
        "model_folder = f\"/content/{model_name}\"\n",
        "ckpt_file = f\"{model_folder}/{model_name}.ckpt\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path /content/$model_name --checkpoint_path $ckpt_file --half"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PpGF4_YP72Xp"
      },
      "outputs": [],
      "source": [
        "#@markdown send to huggingface\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "repo_id = \"\" #@param {type:\"string\"}\n",
        "hf_token = \"\" #@param {type:\"string\"}\n",
        "create_repo(repo_id, private=True, token=hf_token)\n",
        "upload_folder(folder_path=model_folder, path_in_repo=\"\", repo_id=repo_id, commit_message=f\"{model_name}\", token=hf_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}